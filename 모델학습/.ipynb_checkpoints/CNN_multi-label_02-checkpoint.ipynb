{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d2e7f1-5c3f-48d2-9fc9-da7203c63a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be0c880-cc8c-4335-8f0c-1b3224abef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7874 7874\n",
      "1691 1691\n",
      "1687 1687\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "last_path = [[0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1],[1,1,1]]\n",
    "prefix = [f\"{i:03d}\" for i in range(505, 513)]\n",
    "prefix_to_label = dict(zip(prefix, last_path))\n",
    "\n",
    "def process_dataset(root_folder):\n",
    "    image_paths = []\n",
    "    label_data = []\n",
    "\n",
    "    for roots, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                # 파일 이름 분석을 위해 숫자만 추출\n",
    "                prefix = file[0:3]\n",
    "                \n",
    "                # 접두사에 따른 레이블 할당\n",
    "                label = prefix_to_label.get(prefix)\n",
    "                \n",
    "                # 유효한 레이블이 있는 경우에만 리스트에 추가\n",
    "                if label is not None:\n",
    "                    image_paths.append(os.path.join(roots, file))\n",
    "                    label_data.append(label)\n",
    "    \n",
    "    return image_paths, label_data\n",
    "\n",
    "# 각각의 데이터셋에 대해 함수를 호출\n",
    "train_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\training'\n",
    "valid_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\validation'\n",
    "test_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\test'\n",
    "\n",
    "train_image_paths, train_label_data = process_dataset(train_folder)\n",
    "valid_image_paths, valid_label_data = process_dataset(valid_folder)\n",
    "test_image_paths, test_label_data = process_dataset(test_folder)\n",
    "\n",
    "# 필요에 따라 결과를 확인하거나 다른 처리를 수행\n",
    "print(len(train_image_paths), len(train_label_data))\n",
    "print(len(valid_image_paths), len(valid_label_data))\n",
    "print(len(test_image_paths), len(test_label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b87277-a1ac-47bf-80e9-a89fc97f0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "<class 'numpy.ndarray'>\n",
      "(1691, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(7874, 3)\n"
     ]
    }
   ],
   "source": [
    "def resize_img(image_paths):\n",
    "    images_resized = []  # 리사이즈된 이미지를 저장할 리스트\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)  # 각 이미지 경로로부터 이미지를 읽음\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 색상 변환\n",
    "        image_resized = cv2.resize(image, (128, 128))  # 이미지 리사이즈\n",
    "        images_resized.append(image_resized)  # 결과 리스트에 추가\n",
    "    images_resized = np.array(images_resized) / 255.0  # numpy 배열로 변환 및 정규화\n",
    "    return images_resized\n",
    "\n",
    "train_image_resized = resize_img(train_image_paths)\n",
    "valid_image_resized = resize_img(valid_image_paths)\n",
    "test_image_resized = resize_img(test_image_paths)\n",
    "\n",
    "train_label_data = np.array(train_label_data)\n",
    "valid_label_data = np.array(valid_label_data)\n",
    "test_label_data = np.array(test_label_data)\n",
    "\n",
    "print('done')\n",
    "\n",
    "print(type(valid_label_data))  # 데이터 타입 확인\n",
    "if isinstance(valid_label_data, np.ndarray):\n",
    "    print(valid_label_data.shape)  # numpy 배열인 경우, 모양 확인\n",
    "print(type(train_label_data))  # 데이터 타입 확인\n",
    "if isinstance(train_label_data, np.ndarray):\n",
    "    print(train_label_data.shape)  # numpy 배열인 경우, 모양 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b58b1e-89af-4686-a157-4df1dea76dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50v2_RT\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 4, 4, 2048)        23564800  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,344,067\n",
      "Trainable params: 40,298,627\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RN50_2 = tf.keras.applications.resnet_v2.ResNet50V2(weights=\"imagenet\",\n",
    "                                          include_top=False, \n",
    "                                          input_shape=(128, 128, 3))\n",
    "\n",
    "RN50_2.trainable = True # 이미 학습된 가중치만을 사용\n",
    "\n",
    "model = models.Sequential(name=\"ResNet50v2_RT\")\n",
    "model.add(RN50_2)\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(3, activation = 'sigmoid'))  \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd2cc10-f67e-4e52-8968-0e3af48b8a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "247/247 [==============================] - 27s 87ms/step - loss: 0.6995 - accuracy: 0.2654 - val_loss: 0.7488 - val_accuracy: 0.5269\n",
      "Epoch 2/25\n",
      "247/247 [==============================] - 21s 84ms/step - loss: 0.6077 - accuracy: 0.2992 - val_loss: 0.6391 - val_accuracy: 0.3607\n",
      "Epoch 3/25\n",
      "247/247 [==============================] - 21s 84ms/step - loss: 0.5647 - accuracy: 0.3410 - val_loss: 0.5932 - val_accuracy: 0.2703\n",
      "Epoch 4/25\n",
      "247/247 [==============================] - 19s 77ms/step - loss: 0.5338 - accuracy: 0.3697 - val_loss: 0.6616 - val_accuracy: 0.3211\n",
      "Epoch 5/25\n",
      "247/247 [==============================] - 19s 79ms/step - loss: 0.4990 - accuracy: 0.4063 - val_loss: 0.6521 - val_accuracy: 0.3217\n",
      "Epoch 6/25\n",
      "247/247 [==============================] - 23s 94ms/step - loss: 0.4563 - accuracy: 0.4390 - val_loss: 0.6648 - val_accuracy: 0.4021\n",
      "Epoch 7/25\n",
      "247/247 [==============================] - 21s 86ms/step - loss: 0.4132 - accuracy: 0.4691 - val_loss: 0.5780 - val_accuracy: 0.3720\n",
      "Epoch 8/25\n",
      "247/247 [==============================] - 21s 86ms/step - loss: 0.3853 - accuracy: 0.4775 - val_loss: 0.5853 - val_accuracy: 0.3459\n",
      "Epoch 9/25\n",
      "247/247 [==============================] - 19s 76ms/step - loss: 0.3371 - accuracy: 0.5015 - val_loss: 0.6168 - val_accuracy: 0.4453\n",
      "Epoch 10/25\n",
      "247/247 [==============================] - 20s 82ms/step - loss: 0.2921 - accuracy: 0.5328 - val_loss: 0.5713 - val_accuracy: 0.4471\n",
      "Epoch 11/25\n",
      "247/247 [==============================] - 22s 88ms/step - loss: 0.2648 - accuracy: 0.5627 - val_loss: 0.5181 - val_accuracy: 0.3560\n",
      "Epoch 12/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.2122 - accuracy: 0.5638 - val_loss: 0.5976 - val_accuracy: 0.5352\n",
      "Epoch 13/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.2062 - accuracy: 0.5812 - val_loss: 0.6973 - val_accuracy: 0.4205\n",
      "Epoch 14/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.1711 - accuracy: 0.5857 - val_loss: 0.5274 - val_accuracy: 0.5169\n",
      "Epoch 15/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.1387 - accuracy: 0.5822 - val_loss: 0.4573 - val_accuracy: 0.4938\n",
      "Epoch 16/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.1401 - accuracy: 0.6160 - val_loss: 0.4902 - val_accuracy: 0.5174\n",
      "Epoch 17/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.1101 - accuracy: 0.6242 - val_loss: 0.7685 - val_accuracy: 0.4494\n",
      "Epoch 18/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.0998 - accuracy: 0.6092 - val_loss: 0.7037 - val_accuracy: 0.4441\n",
      "Epoch 19/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.0809 - accuracy: 0.6191 - val_loss: 0.7263 - val_accuracy: 0.5234\n",
      "Epoch 20/25\n",
      "247/247 [==============================] - 20s 80ms/step - loss: 0.0937 - accuracy: 0.6726 - val_loss: 0.7178 - val_accuracy: 0.5234\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ac74339970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "model.compile(optimizer = optimizers.Adam(learning_rate = 0.001),  \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_image_resized, \n",
    "          train_label_data, \n",
    "          validation_data=(valid_image_resized, valid_label_data), \n",
    "          epochs=25, \n",
    "          batch_size=32,\n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73b61f-0d5c-43f7-8992-6b354be84c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorGPU",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
