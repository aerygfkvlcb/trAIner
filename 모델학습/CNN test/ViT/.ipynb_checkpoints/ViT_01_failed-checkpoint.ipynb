{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037f84ae-9b6b-4fd8-aa54-ebe006186b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7874 images belonging to 8 classes.\n",
      "Found 1691 images belonging to 8 classes.\n",
      "Found 1687 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from transformers import TFViTModel, ViTConfig\n",
    "\n",
    "# Hugging Face의 transformers에서 ViT 모델과 기능 추출기를 불러옵니다.\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "class ViTForImageClassification(tf.keras.Model):\n",
    "    def __init__(self, num_labels):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier = layers.Dense(num_labels, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.vit(inputs)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(logits)\n",
    "\n",
    "# 데이터셋 경로\n",
    "train_dir = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\training'\n",
    "val_dir = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\validation'\n",
    "test_dir = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\test'\n",
    "\n",
    "# ImageDataGenerator 초기화\n",
    "datagen = ImageDataGenerator(rescale=1./255)  # 이미지를 0과 1 사이의 값으로 정규화\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터셋을 위한 제너레이터 생성\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # ViT 모델에 맞게 이미지 크기 조정\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),  # ViT 모델에 맞게 이미지 크기 조정\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),  # ViT 모델에 맞게 이미지 크기 조정\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9973f0e2-cc5d-49b6-a8cf-257f84ad5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTModel.\n",
      "\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_vi_t_model_1\" (type TFViTModel).\n\nin user code:\n\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 764, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 766, in call  *\n        outputs = self.vit(\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filevskrz7j7.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_file5ffkzam9.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filexuv3jyzp.py\", line 11, in tf__call\n        (batch_size, num_channels, height, width) = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"vit\" \"                 f\"(type TFViTMainLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 764, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 596, in call  *\n            embedding_output = self.embeddings(\n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filexuv3jyzp.py\", line 11, in tf__call\n            (batch_size, num_channels, height, width) = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer \"embeddings\" \"                 f\"(type TFViTEmbeddings).\n        \n        in user code:\n        \n            File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 129, in call  *\n                batch_size, num_channels, height, width = shape_list(pixel_values)\n        \n            ValueError: too many values to unpack (expected 4)\n        \n        \n        Call arguments received by layer \"embeddings\" \"                 f\"(type TFViTEmbeddings):\n          • pixel_values=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n          • interpolate_pos_encoding=None\n          • training=False\n    \n    \n    Call arguments received by layer \"vit\" \"                 f\"(type TFViTMainLayer):\n      • self=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n      • pixel_values=None\n      • head_mask=None\n      • output_attentions=False\n      • output_hidden_states=False\n      • interpolate_pos_encoding=None\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_vi_t_model_1\" (type TFViTModel):\n  • self=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n  • pixel_values=None\n  • head_mask=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • interpolate_pos_encoding=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 맞춤형 모델 정의\u001b[39;00m\n\u001b[0;32m     11\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# 입력 레이어 정의\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m vit_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# ViT 모델의 출력\u001b[39;00m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D()(vit_outputs)  \u001b[38;5;66;03m# 평균 풀링 레이어\u001b[39;00m\n\u001b[0;32m     14\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(num_labels, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)  \u001b[38;5;66;03m# 분류기 레이어\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevskrz7j7.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file601t328z.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mvit, (), \u001b[38;5;28mdict\u001b[39m(pixel_values\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(pixel_values), head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(head_mask), output_attentions\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_attentions), output_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), interpolate_pos_encoding\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(interpolate_pos_encoding), return_dict\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(return_dict), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevskrz7j7.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5ffkzam9.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(pixel_values) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39membeddings, (), \u001b[38;5;28mdict\u001b[39m(pixel_values\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(pixel_values), interpolate_pos_encoding\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(interpolate_pos_encoding), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (head_mask,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexuv3jyzp.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m (batch_size, num_channels, height, width) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_list), (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpatch_embeddings, (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28mdict\u001b[39m(interpolate_pos_encoding\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(interpolate_pos_encoding), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     13\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrepeat, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcls_token,), \u001b[38;5;28mdict\u001b[39m(repeats\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(batch_size), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_vi_t_model_1\" (type TFViTModel).\n\nin user code:\n\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 764, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 766, in call  *\n        outputs = self.vit(\n    File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filevskrz7j7.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_file5ffkzam9.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filexuv3jyzp.py\", line 11, in tf__call\n        (batch_size, num_channels, height, width) = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"vit\" \"                 f\"(type TFViTMainLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 764, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 596, in call  *\n            embedding_output = self.embeddings(\n        File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ajhoo\\AppData\\Local\\Temp\\__autograph_generated_filexuv3jyzp.py\", line 11, in tf__call\n            (batch_size, num_channels, height, width) = ag__.converted_call(ag__.ld(shape_list), (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer \"embeddings\" \"                 f\"(type TFViTEmbeddings).\n        \n        in user code:\n        \n            File \"C:\\Users\\ajhoo\\anaconda3\\envs\\tensorGPU\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 129, in call  *\n                batch_size, num_channels, height, width = shape_list(pixel_values)\n        \n            ValueError: too many values to unpack (expected 4)\n        \n        \n        Call arguments received by layer \"embeddings\" \"                 f\"(type TFViTEmbeddings):\n          • pixel_values=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n          • interpolate_pos_encoding=None\n          • training=False\n    \n    \n    Call arguments received by layer \"vit\" \"                 f\"(type TFViTMainLayer):\n      • self=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n      • pixel_values=None\n      • head_mask=None\n      • output_attentions=False\n      • output_hidden_states=False\n      • interpolate_pos_encoding=None\n      • return_dict=True\n      • training=False\n\n\nCall arguments received by layer \"tf_vi_t_model_1\" (type TFViTModel):\n  • self=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)\n  • pixel_values=None\n  • head_mask=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • interpolate_pos_encoding=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "num_labels = 8  # 예시로 사용할 레이블의 수\n",
    "\n",
    "# ViT 모델 구성 설정\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "config.num_labels = num_labels\n",
    "\n",
    "# ViT 모델 로드\n",
    "vit_model = TFViTModel.from_pretrained('google/vit-base-patch16-224-in21k', config=config)\n",
    "\n",
    "# 맞춤형 모델 정의\n",
    "input_layer = layers.Input(shape=(None, 224, 224, 3))  # 입력 레이어 정의\n",
    "vit_outputs = vit_model(input_layer)[0]  # ViT 모델의 출력\n",
    "x = layers.GlobalAveragePooling1D()(vit_outputs)  # 평균 풀링 레이어\n",
    "output_layer = layers.Dense(num_labels, activation='softmax')(x)  # 분류기 레이어\n",
    "\n",
    "# 최종 모델 생성\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model.summary()\n",
    "\n",
    "# 조기 종료 콜백\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43c18dc-2a64-4c4f-95a8-d2ded0404978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "242/242 [==============================] - 26s 84ms/step - loss: 2.0251 - accuracy: 0.2397 - val_loss: 1.8840 - val_accuracy: 0.2776\n",
      "Epoch 2/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 1.5965 - accuracy: 0.3938 - val_loss: 1.5311 - val_accuracy: 0.4363\n",
      "Epoch 3/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 1.2423 - accuracy: 0.5282 - val_loss: 1.4489 - val_accuracy: 0.4657\n",
      "Epoch 4/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.9914 - accuracy: 0.6225 - val_loss: 1.4459 - val_accuracy: 0.4988\n",
      "Epoch 5/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.7578 - accuracy: 0.7204 - val_loss: 1.3630 - val_accuracy: 0.5475\n",
      "Epoch 6/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.5649 - accuracy: 0.8001 - val_loss: 1.3597 - val_accuracy: 0.5709\n",
      "Epoch 7/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.4426 - accuracy: 0.8498 - val_loss: 1.7216 - val_accuracy: 0.5186\n",
      "Epoch 8/25\n",
      "242/242 [==============================] - 20s 83ms/step - loss: 0.3468 - accuracy: 0.8795 - val_loss: 1.4942 - val_accuracy: 0.5745\n",
      "Epoch 9/25\n",
      "242/242 [==============================] - 19s 80ms/step - loss: 0.2797 - accuracy: 0.9069 - val_loss: 1.4673 - val_accuracy: 0.5871\n",
      "Epoch 10/25\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.2248 - accuracy: 0.9257 - val_loss: 1.5401 - val_accuracy: 0.6082\n",
      "Epoch 11/25\n",
      "242/242 [==============================] - 19s 79ms/step - loss: 0.1819 - accuracy: 0.9416 - val_loss: 1.6332 - val_accuracy: 0.5925\n",
      "Epoch 11: early stopping\n",
      "52/52 [==============================] - 1s 22ms/step - loss: 1.5711 - accuracy: 0.6142\n",
      "\n",
      "테스트 정확도: 0.614182710647583\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "                    epochs = 25, \n",
    "                    verbose=1, \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652674e5-3f26-44f4-ae68-d6397660663f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorGPU",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
