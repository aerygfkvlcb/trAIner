{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be0c880-cc8c-4335-8f0c-1b3224abef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7874 7874\n",
      "1691 1691\n",
      "1687 1687\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "last_path = [[0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,1,1],[1,0,1],[1,1,1]]\n",
    "prefix = [f\"{i:03d}\" for i in range(505, 513)]\n",
    "prefix_to_label = dict(zip(prefix, last_path))\n",
    "\n",
    "def process_dataset(root_folder):\n",
    "    image_paths = []\n",
    "    label_data = []\n",
    "\n",
    "    for roots, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                # 파일 이름 분석을 위해 숫자만 추출\n",
    "                prefix = file[0:3]\n",
    "                \n",
    "                # 접두사에 따른 레이블 할당\n",
    "                label = prefix_to_label.get(prefix)\n",
    "                \n",
    "                # 유효한 레이블이 있는 경우에만 리스트에 추가\n",
    "                if label is not None:\n",
    "                    image_paths.append(os.path.join(roots, file))\n",
    "                    label_data.append(label)\n",
    "    \n",
    "    return image_paths, label_data\n",
    "\n",
    "# 각각의 데이터셋에 대해 함수를 호출\n",
    "train_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\training'\n",
    "valid_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\validation'\n",
    "test_folder = r'E:\\AI\\dataset_skeleton_sep\\face\\BicycleCrunch\\test'\n",
    "\n",
    "train_image_paths, train_label_data = process_dataset(train_folder)\n",
    "valid_image_paths, valid_label_data = process_dataset(valid_folder)\n",
    "test_image_paths, test_label_data = process_dataset(test_folder)\n",
    "\n",
    "# 필요에 따라 결과를 확인하거나 다른 처리를 수행\n",
    "print(len(train_image_paths), len(train_label_data))\n",
    "print(len(valid_image_paths), len(valid_label_data))\n",
    "print(len(test_image_paths), len(test_label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b87277-a1ac-47bf-80e9-a89fc97f0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "<class 'numpy.ndarray'>\n",
      "(1691, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(7874, 3)\n"
     ]
    }
   ],
   "source": [
    "def resize_img(image_paths):\n",
    "    images_resized = []  # 리사이즈된 이미지를 저장할 리스트\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)  # 각 이미지 경로로부터 이미지를 읽음\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 색상 변환\n",
    "        image_resized = cv2.resize(image, (128, 128))  # 이미지 리사이즈\n",
    "        images_resized.append(image_resized)  # 결과 리스트에 추가\n",
    "    images_resized = np.array(images_resized) / 255.0  # numpy 배열로 변환 및 정규화\n",
    "    return images_resized\n",
    "\n",
    "train_image_resized = resize_img(train_image_paths)\n",
    "valid_image_resized = resize_img(valid_image_paths)\n",
    "test_image_resized = resize_img(test_image_paths)\n",
    "\n",
    "train_label_data = np.array(train_label_data)\n",
    "valid_label_data = np.array(valid_label_data)\n",
    "test_label_data = np.array(test_label_data)\n",
    "\n",
    "print('done')\n",
    "\n",
    "print(type(valid_label_data))  # 데이터 타입 확인\n",
    "if isinstance(valid_label_data, np.ndarray):\n",
    "    print(valid_label_data.shape)  # numpy 배열인 경우, 모양 확인\n",
    "print(type(train_label_data))  # 데이터 타입 확인\n",
    "if isinstance(train_label_data, np.ndarray):\n",
    "    print(train_label_data.shape)  # numpy 배열인 경우, 모양 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b58b1e-89af-4686-a157-4df1dea76dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        base_model.trainable = False\n",
    "    for layer in base_model.layers[-9:]:\n",
    "        base_model.trainable = True\n",
    "        \n",
    "    # 모델 구성    \n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=hp.Int('units', min_value=128, max_value=512, step=128), activation = 'relu'))\n",
    "    model.add(layers.Dense(3, activation='sigmoid')) # 멀티라벨 분류를 위한 sigmoid 활성화 함수 사용\n",
    "        \n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='LOG')),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb3ffcd-a165-44de-8950-ab1750644514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 512, 'step': 128, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # 시도할 하이퍼파라미터 조합의 최대 개수\n",
    "    executions_per_trial=1,  # 각 하이퍼파라미터 설정을 평가하기 위해 모델을 훈련시킬 횟수\n",
    "    directory=r'E:\\model\\ResNet_RandomSearch',  # 튜닝 세션의 결과를 저장할 디렉토리 이름\n",
    "    project_name='BicycleCrunch_multilabel_01', # 프로젝트 이름    \n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5a2025-3753-4be9-906b-d0ee04f1da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 04m 08s]\n",
      "val_accuracy: 0.6256653070449829\n",
      "\n",
      "Best val_accuracy So Far: 0.6256653070449829\n",
      "Total elapsed time: 00h 30m 36s\n"
     ]
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', patience=7, mode='min', verbose=1)\n",
    "\n",
    "tuner.search(train_image_resized, \n",
    "             train_label_data, \n",
    "             validation_data=(valid_image_resized, valid_label_data), \n",
    "             epochs=25, \n",
    "             batch_size=32,\n",
    "             callbacks=[earlystopping])\n",
    "\n",
    "\n",
    "# 최적의 하이퍼파라미터 가져오기\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 빌드\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4bd30c-747e-4ca5-a17d-9b812d9507ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value for units is 512\n",
      "The optimal value for learning_rate is 0.00021429980451890025\n",
      "Epoch 1/25\n",
      "247/247 [==============================] - 26s 92ms/step - loss: 0.6841 - accuracy: 0.3366 - val_loss: 0.7340 - val_accuracy: 0.6257\n",
      "Epoch 2/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.5635 - accuracy: 0.3814 - val_loss: 0.7018 - val_accuracy: 0.5305\n",
      "Epoch 3/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.4865 - accuracy: 0.4460 - val_loss: 0.6845 - val_accuracy: 0.3312\n",
      "Epoch 4/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.3959 - accuracy: 0.4816 - val_loss: 0.7344 - val_accuracy: 0.2383\n",
      "Epoch 5/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.3261 - accuracy: 0.5204 - val_loss: 0.5075 - val_accuracy: 0.4228\n",
      "Epoch 6/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.2648 - accuracy: 0.5559 - val_loss: 0.4809 - val_accuracy: 0.4554\n",
      "Epoch 7/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.2118 - accuracy: 0.5721 - val_loss: 0.4863 - val_accuracy: 0.4583\n",
      "Epoch 8/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.1562 - accuracy: 0.5871 - val_loss: 0.6856 - val_accuracy: 0.6339\n",
      "Epoch 9/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.1277 - accuracy: 0.5879 - val_loss: 0.5355 - val_accuracy: 0.4713\n",
      "Epoch 10/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0987 - accuracy: 0.5752 - val_loss: 0.4725 - val_accuracy: 0.4897\n",
      "Epoch 11/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.1023 - accuracy: 0.5906 - val_loss: 0.5125 - val_accuracy: 0.4755\n",
      "Epoch 12/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0865 - accuracy: 0.5989 - val_loss: 0.4881 - val_accuracy: 0.5370\n",
      "Epoch 13/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0608 - accuracy: 0.5900 - val_loss: 0.5909 - val_accuracy: 0.4441\n",
      "Epoch 14/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0728 - accuracy: 0.6016 - val_loss: 0.4847 - val_accuracy: 0.5659\n",
      "Epoch 15/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0470 - accuracy: 0.6278 - val_loss: 0.5858 - val_accuracy: 0.6002\n",
      "Epoch 16/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0530 - accuracy: 0.5982 - val_loss: 0.8053 - val_accuracy: 0.6446\n",
      "Epoch 17/25\n",
      "247/247 [==============================] - 22s 89ms/step - loss: 0.0585 - accuracy: 0.6184 - val_loss: 0.6777 - val_accuracy: 0.5512\n",
      "Epoch 17: early stopping\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.6962 - accuracy: 0.5531\n",
      "\n",
      "테스트 정확도: 0.5530527830123901\n"
     ]
    }
   ],
   "source": [
    "# 모든 최적 하이퍼파라미터 출력\n",
    "for hp in best_hps.values:\n",
    "    print(f\"The optimal value for {hp} is {best_hps.get(hp)}\")\n",
    "    \n",
    "# 모델 훈련\n",
    "model.fit(train_image_resized, \n",
    "          train_label_data, \n",
    "          validation_data=(valid_image_resized, valid_label_data), \n",
    "          epochs=25, \n",
    "          batch_size=32,\n",
    "          callbacks=[earlystopping])\n",
    "\n",
    "# 모델 평가 (테스트 데이터셋)\n",
    "test_loss, test_acc = model.evaluate(test_image_resized, test_label_data)\n",
    "print('\\n테스트 정확도:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73b61f-0d5c-43f7-8992-6b354be84c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorGPU",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
