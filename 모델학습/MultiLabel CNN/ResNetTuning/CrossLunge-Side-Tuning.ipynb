{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14a941f-f0ce-44bc-972e-41221ee3ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import cv2\n",
    "\n",
    "def process_dataset(root_folder):\n",
    "    image_paths = []\n",
    "    label_data = []\n",
    "\n",
    "    for roots, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                # 파일 이름 분석을 위해 숫자만 추출\n",
    "                prefix = file[0:3]\n",
    "\n",
    "                # 접두사에 따른 레이블 할당\n",
    "                label = prefix_to_label.get(prefix)\n",
    "\n",
    "                # 유효한 레이블이 있는 경우에만 리스트에 추가\n",
    "                if label is not None:\n",
    "                    image_paths.append(os.path.join(roots, file))\n",
    "                    label_data.append(label)\n",
    "\n",
    "    return image_paths, label_data\n",
    "\n",
    "\n",
    "def resize_img(image_paths):\n",
    "    images_resized = []  # 리사이즈된 이미지를 저장할 리스트\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)  # 각 이미지 경로로부터 이미지를 읽음\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 색상 변환\n",
    "        image_resized = cv2.resize(image, (128, 128))  # 이미지 리사이즈\n",
    "        images_resized.append(image_resized)  # 결과 리스트에 추가\n",
    "    images_resized = np.array(images_resized) / 255.0  # numpy 배열로 변환 및 정규화\n",
    "    return images_resized\n",
    "\n",
    "\n",
    "def shuffle_data(image_paths, label_data):\n",
    "    # 데이터와 레이블을 같이 섞어줍니다.\n",
    "    indices = np.arange(len(image_paths))\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_image_paths = np.array(image_paths)[indices]\n",
    "    shuffled_label_data = np.array(label_data)[indices]\n",
    "    return shuffled_image_paths, shuffled_label_data\n",
    "\n",
    "\n",
    "def multilabel_train_generator(image_paths, label_data, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    while True:\n",
    "        # 데이터 셔플\n",
    "        image_paths, label_data = shuffle_data(image_paths, label_data)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            # 배치 크기만큼 이미지와 레이블 데이터 로드 및 전처리\n",
    "            batch_image_paths = image_paths[offset:offset + batch_size]\n",
    "            batch_image_labels = label_data[offset:offset + batch_size]\n",
    "\n",
    "            batch_images = resize_img(batch_image_paths)\n",
    "            \n",
    "            for labels in batch_image_labels:\n",
    "                batch_labels.append(labels)\n",
    "\n",
    "            # 배치 데이터 반환\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def multilabel_test_generator(image_paths, label_data, batch_size):\n",
    "    num_samples = len(image_paths)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            # 배치 크기만큼 이미지와 레이블 데이터 로드 및 전처리\n",
    "            batch_image_paths = image_paths[offset:offset + batch_size]\n",
    "            batch_image_labels = label_data[offset:offset + batch_size]\n",
    "\n",
    "            batch_images = resize_img(batch_image_paths)\n",
    "            \n",
    "            for labels in batch_image_labels:\n",
    "                batch_labels.append(labels)\n",
    "\n",
    "            # 배치 데이터 반환\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def get_prefix_to_label(file_path, exercise_name):\n",
    "    data = load_json_file(file_path)\n",
    "    result = (data.get(exercise_name))\n",
    "    prefix = data.get(exercise_name)['prefix']\n",
    "    label = data.get(exercise_name)['last_path']\n",
    "    prefix_to_label = dict(zip(prefix, label))\n",
    "    return prefix_to_label\n",
    "\n",
    "\n",
    "class PerClassAccuracy(Metric):\n",
    "    def __init__(self, name='per_class_accuracy', **kwargs):\n",
    "        super(PerClassAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.acc_per_class = self.add_weight(name='acc', initializer='zeros', shape=(get_label_nums(prefix_to_label),))\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)  # 모델 출력을 이진 분류값으로 변환\n",
    "        y_true = tf.cast(y_true, tf.float32)  # y_true를 float32 타입으로 변환\n",
    "        match = tf.equal(y_true, y_pred)  # 실제 라벨과 예측 라벨이 일치하는지 확인\n",
    "        class_acc = tf.reduce_mean(tf.cast(match, 'float'), axis=0)  # 각 클래스별 정확도 계산\n",
    "        self.acc_per_class.assign(class_acc)  # 계산된 정확도를 저장\n",
    "\n",
    "    def result(self):\n",
    "        return self.acc_per_class\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.acc_per_class.assign(tf.zeros(shape=(get_label_nums(prefix_to_label),)))\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    # 파라미터 추출\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = 32\n",
    "    \n",
    "    # 모델 컴파일과 훈련을 위한 코드 수정\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    \n",
    "    # 모델 훈련 (batch_size를 파라미터로 사용)\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=len(train_image_paths) // batch_size,\n",
    "                        epochs=10,  # 실제로는 적절한 epoch 수 선택\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=len(valid_image_paths) // batch_size,\n",
    "                        verbose=0)  # 로그 출력 제어\n",
    "\n",
    "    # 검증 데이터셋에 대한 평균 val_loss 계산\n",
    "    val_loss = history.history['binary_accuracy'][-1]    \n",
    "    # 시도 결과와 사용된 파라미터 출력\n",
    "    print(f\"시도 결과: val_loss={val_loss}, 사용된 파라미터: learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "    \n",
    "    # hyperopt는 최소값을 찾으므로, val_loss를 그대로 반환\n",
    "    return {'loss': -val_loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def run_trials():\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=10,\n",
    "                trials=trials)\n",
    "\n",
    "    # 최적의 결과와 파라미터 출력\n",
    "    print(\"최적의 결과:\", trials.best_trial['result']['loss'], \"사용된 파라미터:\", trials.best_trial['result']['params'])\n",
    "    return trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341ab011-42eb-4184-83ee-133d76db9a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7343 7343\n",
      "1576 1576\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "json_path = 'E:/AInotes/자세교정/모델학습/label_data.json'\n",
    "exercise_name = 'CrossLunge'\n",
    "direction = 'side' #side\n",
    "train_folder = f'E:/AI/dataset_skeleton_sep/{direction}/{exercise_name}/training'\n",
    "valid_folder = f'E:/AI/dataset_skeleton_sep/{direction}/{exercise_name}/validation'\n",
    "exercise_name = 'CrossLunge-side'.lower()\n",
    "model_path = f'E:/AImodel/models/Multilabel/ResNetTuning/{exercise_name}.h5'\n",
    "\n",
    "prefix_to_label = get_prefix_to_label(json_path, exercise_name)\n",
    "train_image_paths, train_label_data = process_dataset(train_folder)\n",
    "valid_image_paths, valid_label_data = process_dataset(valid_folder)\n",
    "print(len(train_image_paths), len(train_label_data))\n",
    "print(len(valid_image_paths), len(valid_label_data))\n",
    "\n",
    "get_label_nums = lambda x: len(next(iter(x.values())))\n",
    "print(get_label_nums(prefix_to_label))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = multilabel_train_generator(train_image_paths, train_label_data, batch_size)\n",
    "validation_generator = multilabel_test_generator(valid_image_paths, valid_label_data, batch_size)\n",
    "\n",
    "# 모델 구성\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "for layer in base_model.layers:\n",
    "    base_model.trainable = False\n",
    "for layer in base_model.layers[-9:]:\n",
    "    base_model.trainable = True\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(get_label_nums(prefix_to_label), activation='sigmoid'))\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd23afeb-62ed-4416-91f5-f1c4e32d71a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도 결과: val_loss=0.5798112154006958, 사용된 파라미터: learning_rate=0.008420905376828602, batch_size=32                        \n",
      "시도 결과: val_loss=0.5814527273178101, 사용된 파라미터: learning_rate=0.003651081482873659, batch_size=32                        \n",
      "시도 결과: val_loss=0.5828204154968262, 사용된 파라미터: learning_rate=0.005042404421153145, batch_size=32                        \n",
      "시도 결과: val_loss=0.5842339992523193, 사용된 파라미터: learning_rate=0.0008961977797591812, batch_size=32                       \n",
      "시도 결과: val_loss=0.5786715149879456, 사용된 파라미터: learning_rate=0.0028203422119361817, batch_size=32                       \n",
      "시도 결과: val_loss=0.5838234424591064, 사용된 파라미터: learning_rate=0.0034628495545363813, batch_size=32                       \n",
      "시도 결과: val_loss=0.5815437436103821, 사용된 파라미터: learning_rate=0.0007281599608187484, batch_size=32                       \n",
      "시도 결과: val_loss=0.5870151519775391, 사용된 파라미터: learning_rate=0.004699373489846729, batch_size=32                        \n",
      "시도 결과: val_loss=0.5852826237678528, 사용된 파라미터: learning_rate=0.007839353698390686, batch_size=32                        \n",
      "시도 결과: val_loss=0.5827292203903198, 사용된 파라미터: learning_rate=0.0035736655018078606, batch_size=32                       \n",
      "100%|████████████████████████████████████████████| 10/10 [1:05:31<00:00, 393.18s/trial, best loss: -0.5870151519775391]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 최적화할 파라미터의 범위를 정의\u001b[39;00m\n\u001b[0;32m      2\u001b[0m space \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m      4\u001b[0m }\n\u001b[1;32m----> 6\u001b[0m trials \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 각 평가에서의 성능 확인\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m trials\u001b[38;5;241m.\u001b[39mtrials:\n",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m, in \u001b[0;36mrun_trials\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m best \u001b[38;5;241m=\u001b[39m fmin(fn\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[0;32m    160\u001b[0m             space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[0;32m    161\u001b[0m             algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[0;32m    162\u001b[0m             max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    163\u001b[0m             trials\u001b[38;5;241m=\u001b[39mtrials)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# 최적의 결과와 파라미터 출력\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최적의 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trials\u001b[38;5;241m.\u001b[39mbest_trial[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m사용된 파라미터:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trials\n",
      "\u001b[1;31mKeyError\u001b[0m: 'params'"
     ]
    }
   ],
   "source": [
    "# 최적화할 파라미터의 범위를 정의\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 1e-5, 1e-2)\n",
    "}\n",
    "\n",
    "trials = run_trials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95910c-56cf-4c55-bc98-20fd0bcc8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005033729472139928),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "batch_size = 32\n",
    "# 모델 훈련\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=len(train_image_paths) // batch_size,\n",
    "                    epochs=25,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(valid_image_paths) // batch_size,\n",
    "                    callbacks=[earlystopping]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aaa85-f124-44e7-a6b4-b31ceefb0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)\n",
    "print('model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73b61f-0d5c-43f7-8992-6b354be84c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_resized = resize_img(test_image_paths)\n",
    "# # 모델 예측\n",
    "# predictions = model.predict(test_image_resized)\n",
    "\n",
    "# # 임계값 설정 (예: 0.5)\n",
    "# threshold = 0.5\n",
    "# predictions_binary = (predictions > threshold).astype(int)\n",
    "\n",
    "# # 각 레이블에 대한 정확도 계산\n",
    "# accuracy_per_label = np.mean(predictions_binary == test_label_data, axis=0)\n",
    "\n",
    "# # 각 레이블별 정확도 출력\n",
    "# for i, accuracy in enumerate(accuracy_per_label):\n",
    "#     print(f\"레이블 {i}의 정확도: {accuracy}\")\n",
    "\n",
    "# # 전체 정확도도 여전히 중요할 수 있으므로, 이를 계산합니다.\n",
    "# overall_accuracy = np.mean(predictions_binary == test_label_data)\n",
    "# print(f\"전체 정확도: {overall_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5203eb3-dfe7-4aa8-b684-9d72948946ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorGPU",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
