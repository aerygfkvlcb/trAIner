{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b25c67-4964-4065-b05f-d424d4fa639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "003\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "004\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "006\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "007\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "008\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "009\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "010\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "011\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "012\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "013\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "014\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "015\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "016\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "017\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "018\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "019\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "020\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "021\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "022\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "023\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "024\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "025\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "026\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "027\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "028\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "029\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "030\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "031\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "032\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "033\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "034\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "035\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "036\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "037\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "038\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "039\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "040\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "041\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "042\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "043\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "044\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "045\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "046\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "047\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "048\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "049\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "050\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "051\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "053\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "054\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "055\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "056\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "057\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "058\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "059\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "060\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "061\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "062\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "063\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "064\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "065\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "066\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "069\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "070\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "071\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "072\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "075\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "076\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "077\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "078\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "079\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "080\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "081\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "082\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "083\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "084\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "085\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "086\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "087\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "088\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "089\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "090\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "091\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "092\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "093\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "094\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "095\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "096\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "097\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "098\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "099\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "101\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "102\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "104\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "107\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "108\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "109\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "112\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "114\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "116\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "117\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#model_1 transform 61번까지 3920*2160이길래 따로 전처리\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day01\\\\model_1.json'\n",
    "output_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\model_1_trans.json'\n",
    "\n",
    "\n",
    "def transform_json_file(json_path, output_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "    \n",
    "    transformed_data_list = []\n",
    "    for data in data_list:\n",
    "        transformed_data = {}\n",
    "        if 'filename' in data:\n",
    "            transformed_data['filename'] = data['filename']\n",
    "        if 'pts' in data:\n",
    "            if int(data.get('filename')[6:9]) <62:\n",
    "                transformed_pts = {}\n",
    "                for key in data['pts']:\n",
    "                    normalized_x = data['pts'][key]['x'] / 3840.0\n",
    "                    normalized_y = data['pts'][key]['y'] / 2160.0\n",
    "                    transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}' # f'{normalized_x:.6f}' 소수점 6자리 제한\n",
    "                    transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "            else:\n",
    "                transformed_pts = {}\n",
    "                for key in data['pts']:\n",
    "                    normalized_x = data['pts'][key]['x'] / 1920.0\n",
    "                    normalized_y = data['pts'][key]['y'] / 1080.0\n",
    "                    transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                    transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "            transformed_data['pts'] = transformed_pts\n",
    "        if 'img_key' in data:\n",
    "            transformed_data['img_key'] = data['img_key']\n",
    "        \n",
    "        transformed_data_list.append(transformed_data)\n",
    "            \n",
    "    # 결과 JSON 파일 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "transform_json_file(json_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45726450-9e3d-491f-9ee9-3ab52b9ed99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day02'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day02'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17b23e-e822-43c0-b6b6-70959f3e0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day05'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day05'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a756300-375b-4dd3-ae9d-df467c2b17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day09'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day09'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3b55b2-59d7-4a63-9ba6-a5952248e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day10'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day10'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a6b00d-01bd-46f9-974a-5e085469f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day13'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day13'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac942dbd-d501-45af-ba3c-e8e8dfed019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day14'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day14'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414e3f62-bdb5-49e8-a172-afc01a01171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day23'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day23'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4770b4e9-06c5-4f62-a0b7-8e4aa630494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day24'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day24'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5e8b7d3-442c-469a-810c-fd8f7c27cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day27'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day27'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c5a0499-d015-40dc-a68b-6e4e44521b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day28'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day28'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1f478ec-d159-4755-8374-53238ad08e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day33'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day33'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dba1db1-2bb6-412f-a19c-aee277124b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day34'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day34'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56cf166e-5f3b-46df-a832-55e57a5d9a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day35'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day35'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6d2cef6-ef3d-4173-8694-a5273642d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#나머지 모델 transform\n",
    "import os\n",
    "import json\n",
    "\n",
    "# JSON 데이터의 상위 폴더\n",
    "json_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\Day36'\n",
    "out_path = 'E:\\\\AInotes\\\\자세교정\\\\RNN데이터 전처리\\\\keypoint\\\\label_data\\\\Day36'\n",
    "\n",
    "def transform_json_file(json_path, out_path):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data_list = json.load(json_file)\n",
    "        transformed_data_list = []\n",
    "        for items in data_list:\n",
    "            transformed_data = {}\n",
    "            if 'filename' in items:\n",
    "                transformed_data['filename'] = items['filename']\n",
    "            if 'pts' in items:\n",
    "                transformed_pts = {}\n",
    "                for key in items['pts']:\n",
    "                    # data['pts'][key]가 딕셔너리인지 확인\n",
    "                    if isinstance(items['pts'][key], dict):\n",
    "                        normalized_x = items['pts'][key]['x'] / 1920.0\n",
    "                        normalized_y = items['pts'][key]['y'] / 1080.0\n",
    "                        transformed_pts[f'{key}_x'] = f'{normalized_x:.10f}'\n",
    "                        transformed_pts[f'{key}_y'] = f'{normalized_y:.10f}'\n",
    "                    else:\n",
    "                        # data['pts'][key]가 딕셔너리가 아닌 경우의 처리\n",
    "                        print(f\"Unexpected data type in 'pts' for key {key}: {type(data['pts'][key])}\")\n",
    "                transformed_data['pts'] = transformed_pts\n",
    "            if 'img_key' in items:\n",
    "                transformed_data['img_key'] = items['img_key']\n",
    "            transformed_data_list.append(transformed_data)\n",
    "            # 결과 JSON 파일 저장\n",
    "            _, file_name = os.path.split(json_path)\n",
    "            filename_without_extension, _ = os.path.splitext(file_name)\n",
    "            parts = filename_without_extension.split('_')\n",
    "            num = parts[-1]\n",
    "        output_path = os.path.join(out_path, f'model_{num}_trans.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(transformed_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in os.listdir(json_path):\n",
    "    data_path = os.path.join(json_path, data)\n",
    "    if os.path.isfile(data_path) and data_path.endswith('.json'):\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        transform_json_file(data_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a11dbd-0246-4f18-8f57-50b800c9c82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
